#ao text_clustering_with_CNN



This repo re-implemented the algorighm in <b><i>Self-Taught Convolutional Neural Networks for Short Text Clustering<i></b>

The corpus I used is in TURKISH, so I used the word vectors from fastText.

To run this repo, just install requirements and run the <i>main.py<i>.

The goal of this algorithm is to get the deep feature respresentations for input sentences.

Here is an sample output.

`
Average questions length: 8
Max questions length: 23 
Unique tokens count: 5248
The shape of X (9034, 23)
Shape of sentences embedding with tfidf average:  (9034, 300)
The shape of B: (9034, 300)
The shape of B[0] (300,) 
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 23)]              0
_________________________________________________________________
embedding (Embedding)        (None, 23, 300)           1574700
_________________________________________________________________
conv1d (Conv1D)              (None, 23, 300)           450300    
_________________________________________________________________
global_max_pooling1d (Global (None, 300)               0
_________________________________________________________________
dropout (Dropout)            (None, 300)               0
_________________________________________________________________
dense (Dense)                (None, 300)               90300
=================================================================
Total params: 2,115,300
Trainable params: 540,600
Non-trainable params: 1,574,700
_________________________________________________________________
Epoch 1/50
73/73 [==============================] - 3s 35ms/step - loss: 0.5904 - mae: 0.4101 - val_loss: 0.5495 - val_mae: 0.3828
Epoch 2/50
73/73 [==============================] - 2s 29ms/step - loss: 0.5408 - mae: 0.3739 - val_loss: 0.5215 - val_mae: 0.3646
Epoch 3/50
73/73 [==============================] - 2s 30ms/step - loss: 0.5166 - mae: 0.3579 - val_loss: 0.4995 - val_mae: 0.3513
Epoch 4/50
73/73 [==============================] - 2s 30ms/step - loss: 0.4960 - mae: 0.3447 - val_loss: 0.4793 - val_mae: 0.3374
Epoch 5/50
...
73/73 [==============================] - 2s 30ms/step - loss: 0.3082 - mae: 0.2084 - val_loss: 0.2476 - val_mae: 0.1875
Epoch 29/50
73/73 [==============================] - 2s 29ms/step - loss: 0.3054 - mae: 0.2063 - val_loss: 0.2431 - val_mae: 0.1848
Epoch 30/50
73/73 [==============================] - 2s 29ms/step - loss: 0.3031 - mae: 0.2043 - val_loss: 0.2395 - val_mae: 0.1819
Epoch 31/50
73/73 [==============================] - 2s 29ms/step - loss: 0.3007 - mae: 0.2025 - val_loss: 0.2355 - val_mae: 0.1793
Epoch 32/50
73/73 [==============================] - 2s 29ms/step - loss: 0.2982 - mae: 0.2003 - val_loss: 0.2317 - val_mae: 0.1772
Epoch 33/50
73/73 [==============================] - 2s 29ms/step - loss: 0.2964 - mae: 0.1990 - val_loss: 0.2280 - val_mae: 0.1752
Epoch 34/50
73/73 [==============================] - 2s 30ms/step - loss: 0.2940 - mae: 0.1972 - val_loss: 0.2247 - val_mae: 0.1729
Epoch 35/50
73/73 [==============================] - 2s 30ms/step - loss: 0.2916 - mae: 0.1954 - val_loss: 0.2214 - val_mae: 0.1704
Epoch 36/50
73/73 [==============================] - 2s 30ms/step - loss: 0.2906 - mae: 0.1944 - val_loss: 0.2185 - val_mae: 0.1683
Epoch 37/50
73/73 [==============================] - 2s 30ms/step - loss: 0.2883 - mae: 0.1928 - val_loss: 0.2153 - val_mae: 0.1664
Epoch 38/50
73/73 [==============================] - 2s 29ms/step - loss: 0.2871 - mae: 0.1915 - val_loss: 0.2132 - val_mae: 0.1641
Epoch 39/50
73/73 [==============================] - 2s 30ms/step - loss: 0.2852 - mae: 0.1901 - val_loss: 0.2097 - val_mae: 0.1625
Epoch 40/50
73/73 [==============================] - 2s 32ms/step - loss: 0.2835 - mae: 0.1888 - val_loss: 0.2072 - val_mae: 0.1606
Epoch 41/50
73/73 [==============================] - 2s 31ms/step - loss: 0.2823 - mae: 0.1878 - val_loss: 0.2050 - val_mae: 0.1588
Epoch 42/50
73/73 [==============================] - 2s 31ms/step - loss: 0.2806 - mae: 0.1865 - val_loss: 0.2025 - val_mae: 0.1571
Epoch 43/50
73/73 [==============================] - 2s 31ms/step - loss: 0.2793 - mae: 0.1855 - val_loss: 0.1997 - val_mae: 0.1557
Epoch 44/50
73/73 [==============================] - 2s 30ms/step - loss: 0.2783 - mae: 0.1845 - val_loss: 0.1975 - val_mae: 0.1541
Epoch 45/50
73/73 [==============================] - 2s 30ms/step - loss: 0.2765 - mae: 0.1833 - val_loss: 0.1951 - val_mae: 0.1525
Epoch 46/50
73/73 [==============================] - 2s 30ms/step - loss: 0.2752 - mae: 0.1822 - val_loss: 0.1936 - val_mae: 0.1509
Epoch 47/50
73/73 [==============================] - 2s 30ms/step - loss: 0.2745 - mae: 0.1815 - val_loss: 0.1909 - val_mae: 0.1495
Epoch 48/50
73/73 [==============================] - 2s 30ms/step - loss: 0.2736 - mae: 0.1806 - val_loss: 0.1892 - val_mae: 0.1483
Epoch 49/50
73/73 [==============================] - 2s 30ms/step - loss: 0.2721 - mae: 0.1796 - val_loss: 0.1878 - val_mae: 0.1467
Epoch 50/50
73/73 [==============================] - 2s 30ms/step - loss: 0.2712 - mae: 0.1790 - val_loss: 0.1859 - val_mae: 0.1453
The target deep features representation shape: (9034, 300)
Numbers of clusters in label: 18
Similarities among clusters centers:
[[ 1.         -0.05790783  0.1899647   0.18138041 -0.02216757  0.12649047
   0.07539256  0.24349879  0.31759012  0.32772902  0.23568462  0.12968818
   0.29920907  0.29262026  0.39639297  0.20681974  0.24810999  0.11801341]
 [-0.05790783  1.          0.14474807  0.00944427  0.19550672  0.04223995
   0.30419976  0.05309366  0.00225469  0.08158857  0.11802097  0.02024438
   0.17649763  0.16285948 -0.07063961 -0.02969904 -0.15424681  0.62865172]
 [ 0.1899647   0.14474807  1.          0.20017055  0.26063656  0.21693995
   0.26248763  0.25343983  0.25299749  0.39836403  0.23941374  0.06776186
   0.29825575  0.26358897  0.41661425  0.07602315  0.17634106  0.45294364]
 [ 0.18138041  0.00944427  0.20017055  1.          0.06438375  0.06097881
   0.06057687  0.10874476  0.23490033  0.27198231  0.25215357  0.0271248
   0.23541075  0.3930161   0.22731716  0.07585837  0.10256354  0.15472575]
 [-0.02216757  0.19550672  0.26063656  0.06438375  1.          0.13111401
   0.51932302  0.03072339  0.13675074  0.13867516  0.27371304  0.09707098
   0.17893266  0.17979519  0.234514   -0.12285579 -0.01879946  0.22722827]
 [ 0.12649047  0.04223995  0.21693995  0.06097881  0.13111401  1.
   0.14350439  0.30286728  0.27583297  0.41400829  0.35960346  0.24441376
   0.29876559  0.03234421  0.44233472  0.14948847  0.20230046  0.28392082]
 [ 0.07539256  0.30419976  0.26248763  0.06057687  0.51932302  0.14350439
   1.         -0.19402467  0.14706845  0.16910837  0.45611604 -0.06166376
   0.23474473  0.27175456  0.32149751 -0.30263992 -0.11902561  0.23619392]
 [ 0.24349879  0.05309366  0.25343983  0.10874476  0.03072339  0.30286728
  -0.19402467  1.          0.22760697  0.36026944  0.09012225  0.24501937
   0.3101309   0.08471736  0.36394448  0.29840974  0.20623151  0.39656406]
 [ 0.31759012  0.00225469  0.25299749  0.23490033  0.13675074  0.27583297
   0.14706845  0.22760697  1.          0.47142394  0.33420487  0.16481464
   0.32379146  0.15604753  0.51275113  0.19879155  0.21327139  0.26832453]
 [ 0.32772902  0.08158857  0.39836403  0.27198231  0.13867516  0.41400829
   0.16910837  0.36026944  0.47142394  1.          0.50081796  0.12656254
   0.51965768  0.06052301  0.61581199  0.23626706  0.18455137  0.42909904]
 [ 0.23568462  0.11802097  0.23941374  0.25215357  0.27371304  0.35960346
   0.45611604  0.09012225  0.33420487  0.50081796  1.          0.06463229
   0.36406167  0.17446     0.46804823  0.03714385  0.14312733  0.26266956]
 [ 0.12968818  0.02024438  0.06776186  0.0271248   0.09707098  0.24441376
  -0.06166376  0.24501937  0.16481464  0.12656254  0.06463229  1.
   0.27750629  0.00546083  0.24128012  0.28273309  0.24705873  0.151962  ]
 [ 0.29920907  0.17649763  0.29825575  0.23541075  0.17893266  0.29876559
   0.23474473  0.3101309   0.32379146  0.51965768  0.36406167  0.27750629
   1.          0.10185698  0.52832376  0.1877233   0.21285604  0.39697878]
 [ 0.29262026  0.16285948  0.26358897  0.3930161   0.17979519  0.03234421
   0.27175456  0.08471736  0.15604753  0.06052301  0.17446     0.00546083
   0.10185698  1.          0.16879479 -0.0229087   0.05428688  0.22115319]
 [ 0.39639297 -0.07063961  0.41661425  0.22731716  0.234514    0.44233472
   0.32149751  0.36394448  0.51275113  0.61581199  0.46804823  0.24128012
   0.52832376  0.16879479  1.          0.21627102  0.36401841  0.42419644]
 [ 0.20681974 -0.02969904  0.07602315  0.07585837 -0.12285579  0.14948847
  -0.30263992  0.29840974  0.19879155  0.23626706  0.03714385  0.28273309
   0.1877233  -0.0229087   0.21627102  1.          0.4607246   0.14123492]
 [ 0.24810999 -0.15424681  0.17634106  0.10256354 -0.01879946  0.20230046
  -0.11902561  0.20623151  0.21327139  0.18455137  0.14312733  0.24705873
   0.21285604  0.05428688  0.36401841  0.4607246   1.          0.07698398]
 [ 0.11801341  0.62865172  0.45294364  0.15472575  0.22722827  0.28392082
   0.23619392  0.39656406  0.26832453  0.42909904  0.26266956  0.151962
   0.39697878  0.22115319  0.42419644  0.14123492  0.07698398  1.        ]]

`


Ref: 
* https://github.com/jacoxu/STC2
* https://github.com/zqhZY/short_text_cnn_cluster

